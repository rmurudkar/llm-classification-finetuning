{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()\n",
    "\n",
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = df[\"prompt\"][index]\n",
    "response_a = df[\"response_a\"][index]\n",
    "response_b = df[\"response_b\"][index]\n",
    "\n",
    "print(prompt)\n",
    "print(response_a)\n",
    "print(response_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"  # Good model for semantic similarity\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, tokenizer, model, device):\n",
    "    # Tokenize input\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    # Use the [CLS] token embedding as the sentence representation\n",
    "    # or use mean pooling for a potentially better representation\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    \n",
    "    # Mean pooling - taking average of all token embeddings\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    embedding = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "    \n",
    "    return embedding[0]  # Return the first (and only) embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for prompt and response\n",
    "prompt_embedding = get_embedding(prompt, tokenizer, model, device)\n",
    "responsea_embedding = get_embedding(response_a, tokenizer, model, device)\n",
    "responseb_embedding = get_embedding(response_b, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity([prompt_embedding], [responsea_embedding])[0][0]\n",
    "\n",
    "print(f\"Similarity between prompt and response A: {similarity}\")\n",
    "\n",
    "similarity = cosine_similarity([prompt_embedding], [responseb_embedding])[0][0]\n",
    "\n",
    "print(f\"Similarity between prompt and response B: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sentence-transformers/multi-qa-mpnet-base-dot-v1: Specifically optimized for question-answering relevance with strong performance on QA benchmarks.\n",
    "intfloat/e5-large-v2: Designed with a \"query-document\" paradigm that works particularly well for determining if answers are semantically relevant to questions.\n",
    "BAAI/bge-large-en-v1.5: Consistently top performer on retrieval tasks, including question-answering, with strong semantic alignment capabilities.\n",
    "OpenAI text-embedding-3-small: While a general-purpose embedding, it demonstrates excellent performance on question relevance tasks with a good balance of efficiency and quality.\n",
    "sentence-transformers/all-MiniLM-L12-v2: A good compromise between the minimal L6 version and the full MPNet model, with strong QA performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  BAAI/bge-large-en-v1.5\n",
      "Index 1: \n",
      "Prompt:  [\"Construct a rap battle, in the style of Epic Rap Battles of History, with rhyme and meter and personal attacks relevant to each character, between Pikachu and Zeus. Each participant gets 2 verses. There is no announcer and Zeus goes first.\"]\n",
      "Response A:  [\"[Zeus]\\nYo, it's the king of the gods on the mic\\nI control the skies, the thunder, and the lightning strike\\nYou're just a cute little rodent, Pikachu, with a spark\\nBut don't get it twisted, I'll leave you in a blur\\n\\nYou've got electric powers, but I'm the one with the might\\nI can summon lightning bolts and make them ignite\\nYou can't even compare, you're just a mere mouse\\nI'm the ruler of the gods, I'm the one who chooses\\n\\n[Pikachu]\\nYou may be the king of the gods, but I'm the one with the flow\\nI've got rhymes that'll make you bow down, Zeus, I'm the boss\\nYou may have lightning bolts, but I've got electric charm\\nI can shock you with my power, make you feel like you're not in the game\\n\\nYou may be big and bold, but I'm agile and quick\\nI can dodge your bolts and take you down with a single trick\\nYou may be powerful, but I'm the true master of the field\\nI'll leave you in the dust, Zeus, just you wait and see.\"]\n",
      "Response B:  [\"(Verse 1 - Zeus)\\n\\nI'm the king of the gods, ruler of the skies,\\nHere to teach this little rat a lesson he can't despise,\\nZeus, the mighty, ain't no one can defy,\\nAin't no one can stand against, ain't no one even try.\\n\\nI wield the lightning, the thunder, the storm,\\nI'll tear up this battlefield, leave you ragged and forlorn,\\nYou're a rodent, a yellow squeak with no glory,\\nStep aside, Pika-pet, this arena's for true deities.\\n\\n(Verse 1 - Pikachu)\\n\\nPikachu on the mic, I'm a legend in my own right,\\nBeen battling for years, never backing down from a fight,\\nYou call yourself the king? Well, I'm an icon in the game,\\nA force that never fades, while you're just a tired old name.\\n\\nMy thunderbolt will strike, my agility unmatched,\\nYour ancient power's got nothing on the sparks that I'll catch,\\nI've got the love of millions, while you're stuck in the past,\\nMess with this electric mouse, and you'll be shocked by my blast.\\n\\n(Verse 2 - Zeus)\\n\\nHa, you think you're all that, with your fans and your fame,\\nBut when it comes to power, kid, you're just a flickering flame,\\nI've conquered giants, titans, and the realm of the divine,\\nWhat have you done, Pikachu? Fought some kids in your spare time?\\n\\nI've got Olympian strength, no mortal can oppose,\\nWhile you're stuck in a ball, a plaything for kids to dispose,\\nI've stood the test of time, my reign eternal and grand,\\nThis will be a short battle, for it's me who'll make the last stand.\\n\\n(Verse 2 - Pikachu)\\n\\nI've grown beyond a game, I'm a cultural sensation,\\nWhile you're just an old myth, lost to every generation,\\nI've inspired love and loyalty, a symbol of unity,\\nWhile all you've done is betrayed and spread divine cruelty.\\n\\nYour reign is long gone, I'm the one who rules the thunder,\\nI've got the moves and the skills, leaving gods like you to wonder,\\nYou're right, this will be short, but it's me who'll stand tall,\\nFor I'm Pikachu, the electric heart, and I'll never ever fall.\"]\n",
      "Similarity between prompt and response A: 0.8375740647315979, response B: 0.8460932970046997\n",
      "\n",
      "\n",
      "\n",
      "Model:  sentence-transformers/all-MiniLM-L12-v2\n",
      "Index 1: \n",
      "Prompt:  [\"Construct a rap battle, in the style of Epic Rap Battles of History, with rhyme and meter and personal attacks relevant to each character, between Pikachu and Zeus. Each participant gets 2 verses. There is no announcer and Zeus goes first.\"]\n",
      "Response A:  [\"[Zeus]\\nYo, it's the king of the gods on the mic\\nI control the skies, the thunder, and the lightning strike\\nYou're just a cute little rodent, Pikachu, with a spark\\nBut don't get it twisted, I'll leave you in a blur\\n\\nYou've got electric powers, but I'm the one with the might\\nI can summon lightning bolts and make them ignite\\nYou can't even compare, you're just a mere mouse\\nI'm the ruler of the gods, I'm the one who chooses\\n\\n[Pikachu]\\nYou may be the king of the gods, but I'm the one with the flow\\nI've got rhymes that'll make you bow down, Zeus, I'm the boss\\nYou may have lightning bolts, but I've got electric charm\\nI can shock you with my power, make you feel like you're not in the game\\n\\nYou may be big and bold, but I'm agile and quick\\nI can dodge your bolts and take you down with a single trick\\nYou may be powerful, but I'm the true master of the field\\nI'll leave you in the dust, Zeus, just you wait and see.\"]\n",
      "Response B:  [\"(Verse 1 - Zeus)\\n\\nI'm the king of the gods, ruler of the skies,\\nHere to teach this little rat a lesson he can't despise,\\nZeus, the mighty, ain't no one can defy,\\nAin't no one can stand against, ain't no one even try.\\n\\nI wield the lightning, the thunder, the storm,\\nI'll tear up this battlefield, leave you ragged and forlorn,\\nYou're a rodent, a yellow squeak with no glory,\\nStep aside, Pika-pet, this arena's for true deities.\\n\\n(Verse 1 - Pikachu)\\n\\nPikachu on the mic, I'm a legend in my own right,\\nBeen battling for years, never backing down from a fight,\\nYou call yourself the king? Well, I'm an icon in the game,\\nA force that never fades, while you're just a tired old name.\\n\\nMy thunderbolt will strike, my agility unmatched,\\nYour ancient power's got nothing on the sparks that I'll catch,\\nI've got the love of millions, while you're stuck in the past,\\nMess with this electric mouse, and you'll be shocked by my blast.\\n\\n(Verse 2 - Zeus)\\n\\nHa, you think you're all that, with your fans and your fame,\\nBut when it comes to power, kid, you're just a flickering flame,\\nI've conquered giants, titans, and the realm of the divine,\\nWhat have you done, Pikachu? Fought some kids in your spare time?\\n\\nI've got Olympian strength, no mortal can oppose,\\nWhile you're stuck in a ball, a plaything for kids to dispose,\\nI've stood the test of time, my reign eternal and grand,\\nThis will be a short battle, for it's me who'll make the last stand.\\n\\n(Verse 2 - Pikachu)\\n\\nI've grown beyond a game, I'm a cultural sensation,\\nWhile you're just an old myth, lost to every generation,\\nI've inspired love and loyalty, a symbol of unity,\\nWhile all you've done is betrayed and spread divine cruelty.\\n\\nYour reign is long gone, I'm the one who rules the thunder,\\nI've got the moves and the skills, leaving gods like you to wonder,\\nYou're right, this will be short, but it's me who'll stand tall,\\nFor I'm Pikachu, the electric heart, and I'll never ever fall.\"]\n",
      "Similarity between prompt and response A: 0.5509371757507324, response B: 0.5565135478973389\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_overlap(index, model_name):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    prompt = df[\"prompt\"][index]\n",
    "    response_a = df[\"response_a\"][index]\n",
    "    response_b = df[\"response_b\"][index]\n",
    "    winner_model = 'a' if df[\"winner_model_a\"][index] == 1 else 'b'\n",
    "    prompt_embedding = get_embedding(prompt, tokenizer, model, device)\n",
    "    responsea_embedding = get_embedding(response_a, tokenizer, model, device)\n",
    "    responseb_embedding = get_embedding(response_b, tokenizer, model, device)\n",
    "    \n",
    "    similarity_a = cosine_similarity([prompt_embedding], [responsea_embedding])[0][0]\n",
    "    similarity_b = cosine_similarity([prompt_embedding], [responseb_embedding])[0][0]\n",
    "    print(\"Model: \", model_name)\n",
    "    print(\"Index 1: \")\n",
    "    print(\"Prompt: \", prompt)\n",
    "    print(\"Response A: \", response_a)\n",
    "    print(\"Response B: \", response_b)\n",
    "    print(f\"Similarity between prompt and response A: {similarity_a}, response B: {similarity_b}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# semantic_overlap(0, \"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "# semantic_overlap(0, \"intfloat/e5-large-v2\")\n",
    "semantic_overlap(5, \"BAAI/bge-large-en-v1.5\")\n",
    "# semantic_overlap(1, \"OpenAI/text-embedding-3-small\")\n",
    "semantic_overlap(5, \"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_overlap(index, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
